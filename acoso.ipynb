{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "emoji_decode.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJpz-J0IrNw9"
      },
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "class EmojiDecode():\n",
        "\n",
        "  def __init__(self, emoji_path):\n",
        "    emoji_file = open(emoji_path) \n",
        "    self.emojis = json.load(emoji_file)\n",
        "\n",
        "  def multiple_replace(self, di, text):\n",
        "    regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, di.keys())))\n",
        "\n",
        "    return regex.sub(lambda mo: di[mo.string[mo.start():mo.end()]], text) \n",
        "\n",
        "  def filter_emojis(self, by_replace):\n",
        "    return {i : self.emojis[i] for i in by_replace if i in self.emojis.keys()}    \n",
        "\n",
        "  def replace_emoji(self, text):\n",
        "    by_replace = list(set(re.findall(r'[^\\w\\s,]', text)))\n",
        "\n",
        "    emojis = self.filter_emojis(by_replace)\n",
        "\n",
        "    return self.multiple_replace(emojis, text)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdHOWgCil-Ad",
        "outputId": "4bde0fbb-847f-4037-801c-227f271a4d81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "emoji = EmojiDecode('emoticones.json')\n",
        "\n",
        "emoji.replace_emoji('üòÄ hola ü§õ que hace üòÜ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cara sonriente hola pu√±o hacia la izquierda que hace sonriendo entrecerrando los ojos'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46uI5isBehyV"
      },
      "source": [
        "#Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize\n",
        "import string \n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "up3NBbiGez-a"
      },
      "source": [
        "#import dataset\n",
        "import pandas as pd\n",
        "df1 = pd.read_excel(\"dataset_twitter.xlsx\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hv8EdjQdfKHa",
        "outputId": "2da789e5-4d15-4bac-c513-61941583c0c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6SQjTP3e6FE"
      },
      "source": [
        "#Create lists for tweets and label\n",
        "Tweet = []\n",
        "Labels = []\n",
        "\n",
        "for row in df1[\"Tweet\"]:\n",
        "    #tokenize words\n",
        "    words = word_tokenize(row)\n",
        "    #remove punctuations\n",
        "    clean_words = [word.lower() for word in words if word not in set(string.punctuation)]\n",
        "    #remove stop words\n",
        "    english_stops = set(stopwords.words('spanish'))\n",
        "    characters_to_remove = [\"''\",'``',\"rt\",\"https\",\"‚Äô\",\"‚Äú\",\"‚Äù\",\"\\u200b\",\"--\",\"n't\",\"'s\",\"...\",\"//t.c\" ]\n",
        "    clean_words = [word for word in clean_words if word not in english_stops]\n",
        "    clean_words = [word for word in clean_words if word not in set(characters_to_remove)]\n",
        "    #Lematise words\n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\n",
        "    lemma_list = [wordnet_lemmatizer.lemmatize(word) for word in clean_words]\n",
        "    Tweet.append(lemma_list)\n",
        "\n",
        "    for row in df1[\"Y\"]:\n",
        "        Labels.append(row)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1ejNfCefe9N"
      },
      "source": [
        "#Combine lists\n",
        "combined = zip(Tweet, Labels)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZGfN_T5fjYV"
      },
      "source": [
        "#Create bag of words\n",
        "def bag_of_words(words):\n",
        "    return dict([(word, True) for word in words])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEQQ9zupfm9P"
      },
      "source": [
        "#Create new list for modeling\n",
        "Final_Data = []\n",
        "for r, v in combined:\n",
        "    bag_of_words(r)\n",
        "    Final_Data.append((bag_of_words(r),v))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1kZUKhUfnuC",
        "outputId": "d00bf7f6-92b1-4cfa-d6ef-b140b9028302",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import random\n",
        "random.shuffle(Final_Data)\n",
        "print(len(Final_Data))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1065\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_1xZLydfpzm"
      },
      "source": [
        "#Split the data into training and test\n",
        "train_set, test_set = Final_Data[0:746], Final_Data[746:]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cqci3PEf2pN"
      },
      "source": [
        "#Naive Bayes for Unigramsm check accuracy\n",
        "import nltk\n",
        "import collections\n",
        "from nltk.metrics.scores import (accuracy, precision, recall, f_measure) \n",
        "from nltk import metrics"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEndsT5gf4k-",
        "outputId": "df552555-115d-45fa-fbbd-7e175c2b6af4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "refsets = collections. defaultdict(set)\n",
        "testsets = collections.defaultdict(set)\n",
        "\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        " \n",
        "for i, (feats, label) in enumerate(test_set):\n",
        "    refsets[label].add(i)\n",
        "    observed = classifier.classify(feats)\n",
        "    testsets[observed].add(i)\n",
        "\n",
        "print(\"Naive Bayes Performance with Unigrams \")    \n",
        "print(\"Accuracy:\",nltk.classify.accuracy(classifier, test_set))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Naive Bayes Performance with Unigrams \n",
            "Accuracy: 0.6551724137931034\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp9aBZ8Ef6pk",
        "outputId": "ee6702cb-a8cd-4682-f2f9-fa56eab05a96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#Naive Bayes for Unigrams, Recall Measure\n",
        "nb_classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "nbrefset = collections.defaultdict(set)\n",
        "nbtestset = collections.defaultdict(set)\n",
        " \n",
        "for i, (feats, label) in enumerate(test_set):\n",
        "    nbrefset[label].add(i)\n",
        "    observed = nb_classifier.classify(feats)\n",
        "    nbtestset[observed].add(i)\n",
        "print(\"UnigramNB Recall\")\n",
        "print('acoso recall:', recall(nbtestset['acoso'], nbrefset['acoso']))\n",
        "print(\"\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UnigramNB Recall\n",
            "acoso recall: 0.5592105263157895\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G84TgxcSgJvG",
        "outputId": "d9ff1324-dfb3-43de-c27f-fdaadaf8da08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#Find most informative features\n",
        "classifier.show_most_informative_features(n=10)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most Informative Features\n",
            "                  pedazo = True            acoso : noAcos =     12.4 : 1.0\n",
            "                 alguien = True           noAcos : acoso  =      7.8 : 1.0\n",
            "             coeficiente = True            acoso : noAcos =      7.7 : 1.0\n",
            "             intelectual = True            acoso : noAcos =      7.7 : 1.0\n",
            "                    bajo = True            acoso : noAcos =      7.7 : 1.0\n",
            "                     hey = True            acoso : noAcos =      6.5 : 1.0\n",
            "                      ur = True            acoso : noAcos =      6.5 : 1.0\n",
            "               retardado = True            acoso : noAcos =      6.3 : 1.0\n",
            "                       m = True           noAcos : acoso  =      5.8 : 1.0\n",
            "                    co√±o = True            acoso : noAcos =      4.7 : 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUHH82cPgMbE",
        "outputId": "d65b7906-da20-431b-cfba-d8f432a76f90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#Decision Tree for Unigrams\n",
        "from nltk.classify import DecisionTreeClassifier\n",
        "\n",
        "dt_classifier = DecisionTreeClassifier.train(train_set, \n",
        "                                             binary=True, \n",
        "                                             entropy_cutoff=0.8, \n",
        "                                             depth_cutoff=5, \n",
        "                                             support_cutoff=30)\n",
        "refset = collections.defaultdict(set)\n",
        "testset = collections.defaultdict(set)\n",
        " \n",
        "for i, (feats, label) in enumerate(test_set):\n",
        "    refset[label].add(i)\n",
        "    observed = dt_classifier.classify(feats)\n",
        "    testset[observed].add(i)\n",
        "print(\"UnigramDT Recall\")\n",
        "print('acoso recall:', recall(testset['acoso'], refset['acoso']))\n",
        "print(\"\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UnigramDT Recall\n",
            "Bullying recall: 0.6944444444444444\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_VMxJHIgTN0",
        "outputId": "dd24009f-bfcb-4a61-eced-ee91556b92de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#Logisitic Regression for Unigrams\n",
        "from nltk.classify import MaxentClassifier\n",
        "\n",
        "logit_classifier = MaxentClassifier.train(train_set, algorithm='gis', trace=0, max_iter=10, min_lldelta=0.5)\n",
        "\n",
        "for i, (feats, label) in enumerate(test_set):\n",
        "    refset[label].add(i)\n",
        "    observed = logit_classifier.classify(feats)\n",
        "    testset[observed].add(i)\n",
        "print(\"UnigramsLogit Recall\")\n",
        "print('acoso recall:', recall(testset['acoso'], refset['acoso']))\n",
        "print(\"\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UnigramsLogit Recall\n",
            "acoso recall: 0.6608695652173913\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3tjs8T3f---",
        "outputId": "6282f609-486c-4c03-80a0-f0131d9f2a55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Support Vector Machine for Unigrams\n",
        "from nltk.classify import SklearnClassifier\n",
        "from sklearn.svm import SVC\n",
        "SVM_classifier = SklearnClassifier(SVC(), sparse=False).train(train_set)\n",
        " \n",
        "for i, (feats, label) in enumerate(test_set):\n",
        "    refset[label].add(i)\n",
        "    observed = SVM_classifier.classify(feats)\n",
        "    testset[observed].add(i)\n",
        "    \n",
        "print(\"UnigramSVM Recall\")\n",
        "print('acoso recall:', recall(testset['acoso'], refset['acoso']))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UnigramSVM Recall\n",
            "acoso recall: 0.632\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDNmtu64gnET"
      },
      "source": [
        "#Same thing with Bigrams\n",
        "from nltk import bigrams, trigrams\n",
        "from nltk.collocations import BigramCollocationFinder\n",
        "from nltk.metrics import BigramAssocMeasures"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdUeoqtUgpGF"
      },
      "source": [
        "combined = zip(Tweet,Labels)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djO8xwsvgsZd"
      },
      "source": [
        "#Bag of Words of Bigrams\n",
        "def bag_of_bigrams_words(words, score_fn=BigramAssocMeasures.chi_sq, n=200):\n",
        "    bigram_finder = BigramCollocationFinder.from_words(words)  \n",
        "    bigrams = bigram_finder.nbest(score_fn, n)  \n",
        "    return bag_of_words(bigrams)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFI78V9ogt9W"
      },
      "source": [
        "Final_Data2 =[]\n",
        "\n",
        "for z, e in combined:\n",
        "    bag_of_bigrams_words(z)\n",
        "    Final_Data2.append((bag_of_bigrams_words(z),e))"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PPdVOuXgv9t",
        "outputId": "a4e82c1f-f462-403a-fe24-4c8be4a4cc42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Naive Bayes for Bigrams\n",
        "\n",
        "refsets = collections. defaultdict(set)\n",
        "testsets = collections.defaultdict(set)\n",
        "\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "\n",
        " \n",
        "for i, (feats, label) in enumerate(test_set):\n",
        "    refsets[label].add(i)\n",
        "    observed = classifier.classify(feats)\n",
        "    testsets[observed].add(i)\n",
        "\n",
        "\n",
        "print(\"Naive Bayes Performance with Bigrams \")    \n",
        "print(\"Accuracy:\",nltk.classify.accuracy(classifier, test_set))\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Naive Bayes Performance with Bigrams \n",
            "Accuracy: 0.6551724137931034\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHUFBcEVg75L",
        "outputId": "31d6c64d-1aba-41b0-9e24-b2284dad2fa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# In[20]:\n",
        "\n",
        "\n",
        "classifier.show_most_informative_features(n=10)\n",
        "\n",
        "\n",
        "# In[21]:\n",
        "\n",
        "\n",
        "print(\"BigramDT Recall\")\n",
        "print('acoso recall:', recall(testset['acoso'], refset['acoso']))\n",
        "print(\"\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most Informative Features\n",
            "                  pedazo = True            acoso : noAcos =     12.4 : 1.0\n",
            "                 alguien = True           noAcos : acoso  =      7.8 : 1.0\n",
            "             coeficiente = True            acoso : noAcos =      7.7 : 1.0\n",
            "             intelectual = True            acoso : noAcos =      7.7 : 1.0\n",
            "                    bajo = True            acoso : noAcos =      7.7 : 1.0\n",
            "                     hey = True            acoso : noAcos =      6.5 : 1.0\n",
            "                      ur = True            acoso : noAcos =      6.5 : 1.0\n",
            "               retardado = True            acoso : noAcos =      6.3 : 1.0\n",
            "                       m = True           noAcos : acoso  =      5.8 : 1.0\n",
            "                    co√±o = True            acoso : noAcos =      4.7 : 1.0\n",
            "BigramDT Recall\n",
            "Bullying recall: 0.632\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TujTY6ZLhA_m",
        "outputId": "1ad18d42-8a92-4d6e-aefd-0418dea80321",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#Decision Tree for Bigrams\n",
        "from nltk.classify import DecisionTreeClassifier\n",
        "\n",
        "dt_classifier = DecisionTreeClassifier.train(train_set, \n",
        "                                             binary=True, \n",
        "                                             entropy_cutoff=0.8, \n",
        "                                             depth_cutoff=5, \n",
        "                                             support_cutoff=30)\n",
        "refset = collections.defaultdict(set)\n",
        "testset = collections.defaultdict(set)\n",
        " \n",
        "for i, (feats, label) in enumerate(test_set):\n",
        "    refset[label].add(i)\n",
        "    observed = dt_classifier.classify(feats)\n",
        "    testset[observed].add(i)\n",
        "print(\"BigramDT Recall\")\n",
        "print('acoso recall:', recall(testset['acoso'], refset['acoso']))\n",
        "print(\"\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BigramDT Recall\n",
            "Bullying recall: 0.6944444444444444\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxFOSnPuhFrU",
        "outputId": "a2a4f880-be87-4706-ff49-c9874d27c465",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#Logistic Regression for Bigrams\n",
        "from nltk.classify import MaxentClassifier\n",
        "\n",
        "logit_classifier = MaxentClassifier.train(train_set, algorithm='gis', trace=0, max_iter=10, min_lldelta=0.5)\n",
        "\n",
        "for i, (feats, label) in enumerate(test_set):\n",
        "    refset[label].add(i)\n",
        "    observed = logit_classifier.classify(feats)\n",
        "    testset[observed].add(i)\n",
        "print(\"BigramsLogit Recall\")\n",
        "print('acoso recall:', recall(testset['acoso'], refset['acoso']))\n",
        "print(\"\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BigramsLogit Recall\n",
            "Bullying recall: 0.6608695652173913\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuweQIFchK3z",
        "outputId": "aea7e585-46fd-415e-d84e-97753ebc16bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Support Vector Machine for Bigrams\n",
        "from nltk.classify import SklearnClassifier\n",
        "from sklearn.svm import SVC\n",
        "SVM_classifier = SklearnClassifier(SVC(), sparse=False).train(train_set)\n",
        " \n",
        "for i, (feats, label) in enumerate(test_set):\n",
        "    refset[label].add(i)\n",
        "    observed = SVM_classifier.classify(feats)\n",
        "    testset[observed].add(i)\n",
        "    \n",
        "print(\"Bigrams Recall\")\n",
        "print('acoso recall:', recall(testset['acoso'], refset['acoso']))\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bigrams Recall\n",
            "Bullying recall: 0.632\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlWE0JvVhPGP"
      },
      "source": [
        "combined = zip(Tweet,Labels)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJtcwklOhRc7"
      },
      "source": [
        "#Same thing with Trigrams\n",
        "from nltk import bigrams, trigrams\n",
        "from nltk.collocations import TrigramCollocationFinder\n",
        "from nltk.metrics import TrigramAssocMeasures\n",
        "\n",
        "def bag_of_trigrams_words(words, score_fn=TrigramAssocMeasures.chi_sq, n=200):\n",
        "    trigram_finder = TrigramCollocationFinder.from_words(words)  \n",
        "    trigrams = trigram_finder.nbest(score_fn, n)  \n",
        "    return bag_of_words(trigrams)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6SkzxKThTZV",
        "outputId": "6a74eedc-ae7d-469b-840b-ded096fb59c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Final_Data3 =[]\n",
        "\n",
        "for z, e in combined:\n",
        "    bag_of_trigrams_words(z)\n",
        "    Final_Data3.append((bag_of_trigrams_words(z),e))\n",
        "\n",
        "import random\n",
        "random.shuffle(Final_Data3)\n",
        "print(len(Final_Data3))\n",
        "\n",
        "train_set, test_set = Final_Data3[0:747], Final_Data3[747:]"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1065\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1paNqWGUhZuq",
        "outputId": "108a7f29-cfb0-416a-f305-b45293be0ccd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "#Naive Bayes for Trigrams\n",
        "import nltk\n",
        "import collections\n",
        "from nltk.metrics.scores import (accuracy, precision, recall, f_measure) \n",
        "from nltk import metrics\n",
        "\n",
        "\n",
        "refsets = collections. defaultdict(set)\n",
        "testsets = collections.defaultdict(set)\n",
        "\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "\n",
        " \n",
        "for i, (feats, label) in enumerate(test_set):\n",
        "    refsets[label].add(i)\n",
        "    observed = classifier.classify(feats)\n",
        "    testsets[observed].add(i)\n",
        "\n",
        "\n",
        "print(\"Naive Bayes Performance with Trigrams \")    \n",
        "print(\"Accuracy:\",nltk.classify.accuracy(classifier, test_set))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Naive Bayes Performance with Trigrams \n",
            "Accuracy: 0.6132075471698113\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRunAKoIhfPE",
        "outputId": "818d0f6e-bf91-420e-d31d-2d11ae280268",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('acoso precision:', precision(refsets['acoso'], testsets['acoso']))\n",
        "print('acoso recall:', recall(refsets['acoso'], testsets['acoso']))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "acoso precision: 0.84\n",
            "acoso recall: 0.15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSJaKtSEhh9E",
        "outputId": "aab53b2e-2f97-4e49-aa34-a7bd9b86ce80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "classifier.show_most_informative_features(n=10)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most Informative Features\n",
            "('bajo', 'coeficiente', 'intelectual') = True            acoso : noAcos =      8.0 : 1.0\n",
            "       ('i', 'don', 't') = True            acoso : noAcos =      2.7 : 1.0\n",
            "   ('usted', 'don', 't') = True            acoso : noAcos =      2.7 : 1.0\n",
            "('perras', 'fugly', 'tabaquismo') = True            acoso : noAcos =      1.6 : 1.0\n",
            "('low-iq-arabistas', 'tratan', 'borrones') = True            acoso : noAcos =      1.6 : 1.0\n",
            "('identificar', 'w', 'arabismo') = True            acoso : noAcos =      1.6 : 1.0\n",
            "('minor√≠as', 'racistas', 'don') = True            acoso : noAcos =      1.6 : 1.0\n",
            "('individuo', 'bajo', 'coeficiente') = True            acoso : noAcos =      1.6 : 1.0\n",
            "('racistas', 'don', \"'t\") = True            acoso : noAcos =      1.6 : 1.0\n",
            "('coeficiente', 'intelectual', 'bajo') = True            acoso : noAcos =      1.6 : 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5Eo1Xhchl5E",
        "outputId": "b5c75ec2-77ba-459f-dbdd-35d26609313f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#Decision Tree for Trigrams\n",
        "from nltk.classify import DecisionTreeClassifier\n",
        "\n",
        "dt_classifier = DecisionTreeClassifier.train(train_set, \n",
        "                                             binary=True, \n",
        "                                             entropy_cutoff=0.8, \n",
        "                                             depth_cutoff=5, \n",
        "                                             support_cutoff=30)\n",
        "refset = collections.defaultdict(set)\n",
        "testset = collections.defaultdict(set)\n",
        " \n",
        "for i, (feats, label) in enumerate(test_set):\n",
        "    refset[label].add(i)\n",
        "    observed = dt_classifier.classify(feats)\n",
        "    testset[observed].add(i)\n",
        "print(\"TrigramDT Recall\")\n",
        "print('acoso recall:', recall(testset['acoso'], refset['acoso']))\n",
        "print(\"\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TrigramDT Recall\n",
            "acoso recall: 0.9\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LplDucYlhxEz",
        "outputId": "6c3f16e6-9258-427b-c894-4d7a6edae7d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#Logistic Regression for Trigrams\n",
        "from nltk.classify import MaxentClassifier\n",
        "\n",
        "logit_classifier = MaxentClassifier.train(train_set, algorithm='gis', trace=0, max_iter=10, min_lldelta=0.5)\n",
        "\n",
        "for i, (feats, label) in enumerate(test_set):\n",
        "    refset[label].add(i)\n",
        "    observed = logit_classifier.classify(feats)\n",
        "    testset[observed].add(i)\n",
        "print(\"TrigramsLogit Recall\")\n",
        "print('acoso recall:', recall(testset['acoso'], refset['acoso']))\n",
        "print(\"\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TrigramsLogit Recall\n",
            "Bullying recall: 0.84\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gK6GH1vTh1sW",
        "outputId": "5be71483-3c6c-4752-feff-2f5309386aad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Support Vector Machine for Trigrams\n",
        "from nltk.classify import SklearnClassifier\n",
        "from sklearn.svm import SVC\n",
        "SVM_classifier = SklearnClassifier(SVC(), sparse=False).train(train_set)\n",
        " \n",
        "for i, (feats, label) in enumerate(test_set):\n",
        "    refset[label].add(i)\n",
        "    observed = SVM_classifier.classify(feats)\n",
        "    testset[observed].add(i)\n",
        "    \n",
        "print(\"Trigrams Recall\")\n",
        "print('acoso recall:', recall(testset['acoso'], refset['acoso']))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trigrams Recall\n",
            "Bullying recall: 0.84\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9YYAvpdh47c"
      },
      "source": [
        "combined = zip(Tweet,Labels)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFrq4Qbnh8P1"
      },
      "source": [
        "#Combine both unigrams, bigrams, and trigrams\n",
        "\n",
        "# Import Bigram metrics - we will use these to identify the top 200 bigrams\n",
        "def bigrams_words(words, score_fn=BigramAssocMeasures.chi_sq,\n",
        "n=200):\n",
        "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
        "    bigrams = bigram_finder.nbest(score_fn, n)\n",
        "    return bigrams\n",
        "\n",
        "from nltk.collocations import TrigramCollocationFinder\n",
        "\n",
        "# Import Bigram metrics - we will use these to identify the top 200 trigrams \n",
        "from nltk.metrics import TrigramAssocMeasures\n",
        "\n",
        "def trigrams_words(words, score_fn=TrigramAssocMeasures.chi_sq,\n",
        "n=200):\n",
        "    trigram_finder = TrigramCollocationFinder.from_words(words)\n",
        "    trigrams = trigram_finder.nbest(score_fn, n)\n",
        "    return trigrams\n",
        "\n",
        "#bag of ngrams\n",
        "def bag_of_Ngrams_words(words):\n",
        "    bigramBag = bigrams_words(words)\n",
        "    \n",
        "    #The following two for loops convert tuple into string\n",
        "    for b in range(0,len(bigramBag)):\n",
        "        bigramBag[b]=' '.join(bigramBag[b])\n",
        "   \n",
        "    trigramBag = trigrams_words(words)\n",
        "    for t in range(0,len(trigramBag)):\n",
        "        trigramBag[t]=' '.join(trigramBag[t])\n",
        "\n",
        "    return bag_of_words(trigramBag + bigramBag + words)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EzxIjZ6h-zD"
      },
      "source": [
        "Final_Data4 =[]\n",
        "\n",
        "for z, e in combined:\n",
        "    bag_of_Ngrams_words(z)\n",
        "    Final_Data4.append((bag_of_Ngrams_words(z),e))"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUJGxG_yiBes",
        "outputId": "4c91b55b-779b-43ad-9906-379dc593b556",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "\n",
        "import random\n",
        "random.shuffle(Final_Data4)\n",
        "print(len(Final_Data4))\n",
        "\n",
        "train_set, test_set = Final_Data4[0:747], Final_Data4[747:]\n",
        "\n",
        "import nltk\n",
        "import collections\n",
        "from nltk.metrics.scores import (accuracy, precision, recall, f_measure) \n",
        "from nltk import metrics\n",
        "#Naive Bayes for Ngrams\n",
        "\n",
        "refsets = collections. defaultdict(set)\n",
        "testsets = collections.defaultdict(set)\n",
        "\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "\n",
        " \n",
        "for i, (feats, label) in enumerate(test_set):\n",
        "    refsets[label].add(i)\n",
        "    observed = classifier.classify(feats)\n",
        "    testsets[observed].add(i)\n",
        "\n",
        "\n",
        "print(\"Naive Bayes Performance with Ngrams \")    \n",
        "print(\"Accuracy:\",nltk.classify.accuracy(classifier, test_set))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1065\n",
            "Naive Bayes Performance with Ngrams \n",
            "Accuracy: 0.6666666666666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykGvdM1NiE9t",
        "outputId": "8556b6a4-938b-4e05-db4a-b1e6d02dad5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "classifier.show_most_informative_features(n=10)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most Informative Features\n",
            "                  in√∫til = True            acoso : noAcos =      9.4 : 1.0\n",
            "                pat√©tico = True            acoso : noAcos =      9.4 : 1.0\n",
            "                 alguien = True           noAcos : acoso  =      7.4 : 1.0\n",
            "                      ur = True            acoso : noAcos =      7.4 : 1.0\n",
            "                    bajo = True            acoso : noAcos =      7.2 : 1.0\n",
            "                      .i = True           noAcos : acoso  =      7.0 : 1.0\n",
            "               retardado = True            acoso : noAcos =      6.8 : 1.0\n",
            "bajo coeficiente intelectual = True            acoso : noAcos =      6.6 : 1.0\n",
            "        bajo coeficiente = True            acoso : noAcos =      6.6 : 1.0\n",
            "               prejuicio = True           noAcos : acoso  =      6.5 : 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnvyAwhriH5W",
        "outputId": "427113a2-8c8b-4fdf-f49e-97cb2a24151b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('acoso precision:', precision(refsets['acoso'], testsets['acoso']))\n",
        "print('acoso recall:', recall(refsets['acoso'], testsets['acoso']))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bullying precision: 0.5617283950617284\n",
            "bullying recall: 0.7222222222222222\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhFBa5hTiMkD",
        "outputId": "2b9bef90-16ca-43bb-b416-f6cd942df53c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#Decision Tree for Ngrams\n",
        "from nltk.classify import DecisionTreeClassifier\n",
        "\n",
        "dt_classifier = DecisionTreeClassifier.train(train_set, \n",
        "                                             binary=True, \n",
        "                                             entropy_cutoff=0.8, \n",
        "                                             depth_cutoff=5, \n",
        "                                             support_cutoff=30)\n",
        "refset = collections.defaultdict(set)\n",
        "testset = collections.defaultdict(set)\n",
        " \n",
        "for i, (feats, label) in enumerate(test_set):\n",
        "    refset[label].add(i)\n",
        "    observed = dt_classifier.classify(feats)\n",
        "    testset[observed].add(i)\n",
        "print(\"NgramDT Recall\")\n",
        "print('acoso recall:', recall(testset['acoso'], refset['acoso']))\n",
        "print(\"\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NgramDT Recall\n",
            "acoso recall: 0.6714285714285714\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tJ7EVq8iRD9",
        "outputId": "f79d5a92-18fc-4662-dff4-38cd8ea7101e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#Logistic Regression for Ngrams\n",
        "from nltk.classify import MaxentClassifier\n",
        "\n",
        "logit_classifier = MaxentClassifier.train(train_set, algorithm='gis', trace=0, max_iter=10, min_lldelta=0.5)\n",
        "\n",
        "for i, (feats, label) in enumerate(test_set):\n",
        "    refset[label].add(i)\n",
        "    observed = logit_classifier.classify(feats)\n",
        "    testset[observed].add(i)\n",
        "print(\"NgramsLogit Recall\")\n",
        "print('acoso recall:', recall(testset['acoso'], refset['acoso']))\n",
        "print(\"\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NgramsLogit Recall\n",
            "acoso recall: 0.5964912280701754\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twKvaycfiVAe",
        "outputId": "d91eb411-5001-41b8-caef-e154087a8168",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Support Vector Machine for Ngrams\n",
        "from nltk.classify import SklearnClassifier\n",
        "from sklearn.svm import SVC\n",
        "SVM_classifier = SklearnClassifier(SVC(), sparse=False).train(train_set)\n",
        " \n",
        "for i, (feats, label) in enumerate(test_set):\n",
        "    refset[label].add(i)\n",
        "    observed = SVM_classifier.classify(feats)\n",
        "    testset[observed].add(i)\n",
        "    \n",
        "print(\"Trigrams Recall\")\n",
        "print('acoso recall:', recall(testset['acoso'], refset['acoso']))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trigrams Recall\n",
            "acoso recall: 0.5714285714285714\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVUEe5kpiYRD",
        "outputId": "825902aa-4e75-443a-c88c-7cb57b67f4a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "train_set, test_set = Final_Data[0:747], Final_Data[747:]\n",
        "\n",
        "import nltk\n",
        "import collections\n",
        "from nltk.metrics.scores import (accuracy, precision, recall, f_measure)\n",
        "nb_classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "nb_classifier.show_most_informative_features(10)\n",
        "\n",
        "from nltk.classify.util import accuracy\n",
        "print(accuracy(nb_classifier, test_set))\n",
        "\n",
        "refsets = collections.defaultdict(set)\n",
        "testsets = collections.defaultdict(set)\n",
        "    \n",
        "for i, (Final_Data, label) in enumerate(test_set):\n",
        "    refsets[label].add(i)\n",
        "    observed = nb_classifier.classify(Final_Data)\n",
        "    testsets[observed].add(i)\n",
        "    \n",
        "print('acoso precision:', precision(refsets['acoso'], testsets['acoso']))\n",
        "print('acoso recall:', recall(refsets['acoso'], testsets['acoso']))\n",
        "print('acoso F-measure:', f_measure(refsets['acoso'], testsets['acoso']))\n",
        "print('noAcoso precision:', precision(refsets['noAcoso'], testsets['noAcoso']))\n",
        "print('noAcoso recall:', recall(refsets['noAcoso'], testsets['noAcoso']))\n",
        "print('noAcoso F-measure:', f_measure(refsets['noAcoso'], testsets['noAcoso']))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most Informative Features\n",
            "                  pedazo = True            acoso : noAcos =     12.5 : 1.0\n",
            "                 alguien = True           noAcos : acoso  =      7.8 : 1.0\n",
            "             coeficiente = True            acoso : noAcos =      7.8 : 1.0\n",
            "             intelectual = True            acoso : noAcos =      7.8 : 1.0\n",
            "                    bajo = True            acoso : noAcos =      7.7 : 1.0\n",
            "                     hey = True            acoso : noAcos =      6.5 : 1.0\n",
            "                      ur = True            acoso : noAcos =      6.5 : 1.0\n",
            "               retardado = True            acoso : noAcos =      6.3 : 1.0\n",
            "                       m = True           noAcos : acoso  =      5.7 : 1.0\n",
            "                    co√±o = True            acoso : noAcos =      4.7 : 1.0\n",
            "0.6540880503144654\n",
            "acoso precision: 0.5584415584415584\n",
            "acoso recall: 0.671875\n",
            "acoso F-measure: 0.6099290780141844\n",
            "noAcoso precision: 0.7439024390243902\n",
            "noAcoso recall: 0.6421052631578947\n",
            "noAcoso F-measure: 0.6892655367231638\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6jppw71ipUF",
        "outputId": "ce68ba5a-7fb4-445d-d10f-b173c30589da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import collections\n",
        "from nltk import metrics\n",
        "from nltk.metrics.scores import (accuracy, precision, recall, f_measure)\n",
        "from nltk.classify import DecisionTreeClassifier\n",
        "from nltk.classify.util import accuracy\n",
        "dt_classifier = DecisionTreeClassifier.train(train_set, \n",
        "                                             binary=True, \n",
        "                                             entropy_cutoff=0.8, \n",
        "                                             depth_cutoff=5, \n",
        "                                             support_cutoff=30)\n",
        "from nltk.classify.util import accuracy\n",
        "print(accuracy(dt_classifier, test_set))\n",
        "\n",
        "refsets = collections.defaultdict(set)\n",
        "testsets = collections.defaultdict(set)\n",
        "    \n",
        "for i, (Final_Data, label) in enumerate(test_set):\n",
        "    refsets[label].add(i)\n",
        "    observed = dt_classifier.classify(Final_Data)\n",
        "    testsets[observed].add(i)\n",
        "    \n",
        "print('acoso precision:', precision(refsets['acoso'], testsets['acoso']))\n",
        "print('acoso recall:', recall(refsets['acoso'], testsets['acoso']))\n",
        "print('acoso F-measure:', f_measure(refsets['acoso'], testsets['acoso']))\n",
        "print('non-bullying precision:', precision(refsets['noAcoso'], testsets['noAcoso']))\n",
        "print('non-bullying recall:', recall(refsets['noAcoso'], testsets['noAcoso']))\n",
        "print('non-bullying F-measure:', f_measure(refsets['noAcoso'], testsets['noAcoso']))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6855345911949685\n",
            "acoso precision: 0.6944444444444444\n",
            "acoso recall: 0.390625\n",
            "acoso F-measure: 0.5\n",
            "non-bullying precision: 0.6829268292682927\n",
            "non-bullying recall: 0.8842105263157894\n",
            "non-bullying F-measure: 0.7706422018348625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKX4qJTmiqAS",
        "outputId": "40cd66b7-8415-4520-aae0-f5aacb6a5b9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "#Create Logistic Regression model to compare\n",
        "from nltk.classify import MaxentClassifier\n",
        "import collections\n",
        "from nltk.metrics.scores import (accuracy, precision, recall, f_measure)\n",
        "\n",
        "logit_classifier = MaxentClassifier.train(train_set, algorithm='gis', trace=0, max_iter=10, min_lldelta=0.5)\n",
        " \n",
        "for i, (Final_Data, label) in enumerate(test_set):\n",
        "    refsets[label].add(i)\n",
        "    observed = logit_classifier.classify(Final_Data)\n",
        "    testsets[observed].add(i)\n",
        "  \n",
        "print('pos precision:', precision(refsets['acoso'], testsets['noAcoso']))\n",
        "print('pos recall:', recall(refsets['acoso'], testsets['noAcoso']))\n",
        "print('pos F-measure:', f_measure(refsets['acoso'], testsets['noAcoso']))\n",
        "print('neg precision:', precision(refsets['noAcoso'], testsets['noAcoso']))\n",
        "print('neg recall:', recall(refsets['noAcoso'], testsets['noAcoso']))\n",
        "print('neg F-measure:', f_measure(refsets['noAcoso'], testsets['noAcoso']))\n",
        "\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos precision: 0.3357664233576642\n",
            "pos recall: 0.71875\n",
            "pos F-measure: 0.4577114427860696\n",
            "neg precision: 0.6642335766423357\n",
            "neg recall: 0.9578947368421052\n",
            "neg F-measure: 0.7844827586206897\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaVOVoihisN7",
        "outputId": "71cd531c-e92f-42d2-e6c2-b5279751927a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# SVM model\n",
        "\n",
        "from nltk.classify import SklearnClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "SVM_classifier = SklearnClassifier(SVC(), sparse=False).train(train_set)\n",
        "\n",
        "for i, (Final_Data, label) in enumerate(test_set):\n",
        "    refsets[label].add(i)\n",
        "    observed = SVM_classifier.classify(Final_Data)\n",
        "    testsets[observed].add(i)\n",
        "    \n",
        "print('pos precision:', precision(refsets['acoso'], testsets['acoso']))\n",
        "print('pos recall:', recall(refsets['acoso'], testsets['acoso']))\n",
        "print('pos F-measure:', f_measure(refsets['acoso'], testsets['acoso']))\n",
        "print('neg precision:', precision(refsets['noAcoso'], testsets['noAcoso']))\n",
        "print('neg recall:', recall(refsets['noAcoso'], testsets['noAcoso']))\n",
        "print('neg F-measure:', f_measure(refsets['noAcoso'], testsets['noAcoso']))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos precision: 0.632\n",
            "pos recall: 0.6171875\n",
            "pos F-measure: 0.6245059288537549\n",
            "neg precision: 0.6618181818181819\n",
            "neg recall: 0.9578947368421052\n",
            "neg F-measure: 0.7827956989247312\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H240iiLzixQU",
        "outputId": "95d3d9c1-d28b-40b4-a32d-53aa832f2a46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "zl = zip(Tweet,Labels)\n",
        "\n",
        "#define a bag_of_words function to return word, True.\n",
        "\n",
        "def bag_of_words(words):\n",
        "    return dict([(word, True) for word in words])\n",
        "\n",
        "def bag_of_words_not_in_set(words, badwords):\n",
        "    return bag_of_words(set(words) - set(badwords))\n",
        "\n",
        "# Define another function that will return words that are in words, but not in badwords\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#define a bag_of_non_stopwords function to return word, True.\n",
        "\n",
        "def bag_of_non_stopwords(words, stopfile='spanish'):\n",
        "    badwords = stopwords.words(stopfile)\n",
        "    return bag_of_words_not_in_set(words, badwords)\n",
        "\n",
        "from nltk.collocations import BigramCollocationFinder\n",
        "\n",
        "# Import Bigram metrics - we will use these to identify the top 200 bigrams\n",
        "from nltk.metrics import BigramAssocMeasures\n",
        "\n",
        "def bag_of_bigrams_words(words, score_fn=BigramAssocMeasures.chi_sq, n=200):\n",
        "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
        "    bigrams = bigram_finder.nbest(score_fn, n)\n",
        "    return bag_of_words(bigrams)\n",
        "    \n",
        "    bigrams = bag_of_bigrams_words(words)\n",
        "\n",
        "#Creating our unigram featureset dictionary for modeling\n",
        "\n",
        "Final_Data = []\n",
        "\n",
        "for k, v in zl:\n",
        "    bag_of_bigrams_words(k)\n",
        "    Final_Data.append((bag_of_bigrams_words(k),v))\n",
        "\n",
        "import random\n",
        "random.shuffle(Final_Data)\n",
        "\n",
        "#splits the data around 70% of 500 *350 reviews* for both testing and training\n",
        "\n",
        "train_set, test_set = Final_Data[0:778], Final_Data[778:]\n",
        "\n",
        "#Now we will calculate accuracy, precision, recall, and f-measure using Naives Bayes classifier\n",
        "\n",
        "import nltk\n",
        "import collections\n",
        "from nltk.metrics.scores import (accuracy, precision, recall, f_measure)\n",
        "nb_classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "nb_classifier.show_most_informative_features(10)\n",
        "\n",
        "from nltk.classify.util import accuracy\n",
        "print(accuracy(nb_classifier, test_set))\n",
        "\n",
        "refsets = collections.defaultdict(set)\n",
        "testsets = collections.defaultdict(set)\n",
        "    \n",
        "for i, (Final_Data, label) in enumerate(test_set):\n",
        "    refsets[label].add(i)\n",
        "    observed = nb_classifier.classify(Final_Data)\n",
        "    testsets[observed].add(i)\n",
        "    \n",
        "print('acoso precision:', precision(refsets['acoso'], testsets['acoso']))\n",
        "print('acoso recall:', recall(refsets['acoso'], testsets['acoso']))\n",
        "print('acoso F-measure:', f_measure(refsets['acoso'], testsets['acoso']))\n",
        "print('noAcoso precision:', precision(refsets['noAcoso'], testsets['noAcoso']))\n",
        "print('noAcoso recall:', recall(refsets['noAcoso'], testsets['noAcoso']))\n",
        "print('noAcoso F-measure:', f_measure(refsets['noAcoso'], testsets['noAcoso']))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most Informative Features\n",
            "    ('pedazo', 'mierda') = True            acoso : noAcos =     10.8 : 1.0\n",
            " ('bajo', 'coeficiente') = True            acoso : noAcos =     10.0 : 1.0\n",
            "     ('usted', 'idiota') = True            acoso : noAcos =      8.8 : 1.0\n",
            "    ('in√∫til', 'pedazo') = True            acoso : noAcos =      8.8 : 1.0\n",
            "('coeficiente', 'intelectual') = True            acoso : noAcos =      6.9 : 1.0\n",
            "          ('puede', 't') = True            acoso : noAcos =      4.6 : 1.0\n",
            "         ('debe', 'ser') = True            acoso : noAcos =      4.6 : 1.0\n",
            "              ('i', 'm') = True           noAcos : acoso  =      3.7 : 1.0\n",
            "        ('puto', 'co√±o') = True            acoso : noAcos =      3.4 : 1.0\n",
            "          ('tal', 'vez') = True            acoso : noAcos =      2.6 : 1.0\n",
            "0.662020905923345\n",
            "acoso precision: 0.676056338028169\n",
            "acoso recall: 0.39344262295081966\n",
            "acoso F-measure: 0.4974093264248705\n",
            "noAcoso precision: 0.6574074074074074\n",
            "noAcoso recall: 0.8606060606060606\n",
            "noAcoso F-measure: 0.7454068241469816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAxNrhvHizy2"
      },
      "source": [
        "def bigrams_words(words, score_fn=BigramAssocMeasures.chi_sq,\n",
        "n=200):\n",
        "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
        "    bigrams = bigram_finder.nbest(score_fn, n)\n",
        "    return bigrams\n",
        "\n",
        "from nltk.collocations import TrigramCollocationFinder\n",
        "\n",
        "# Import Bigram metrics - we will use these to identify the top 200 bigrams\n",
        "from nltk.metrics import TrigramAssocMeasures\n",
        "\n",
        "def trigrams_words(words, score_fn=TrigramAssocMeasures.chi_sq,\n",
        "n=200):\n",
        "    trigram_finder = TrigramCollocationFinder.from_words(words)\n",
        "    trigrams = trigram_finder.nbest(score_fn, n)\n",
        "    return trigrams\n",
        "\n",
        "\n",
        "def bag_of_Ngrams_words(words):\n",
        "    bigramBag = bigrams_words(words)\n",
        "    \n",
        "    #The following two for loops convert tuple into string\n",
        "    for b in range(0,len(bigramBag)):\n",
        "        bigramBag[b]=' '.join(bigramBag[b])\n",
        "   \n",
        "    trigramBag = trigrams_words(words)\n",
        "    for t in range(0,len(trigramBag)):\n",
        "        trigramBag[t]=' '.join(trigramBag[t])\n",
        "\n",
        "    return bag_of_words(trigramBag + bigramBag + words)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkF5e6bSi3WM",
        "outputId": "d9567199-0dc4-4354-9143-1733a62165eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "zl = zip(Tweet,Labels)\n",
        "\n",
        "Final_Data = []\n",
        "\n",
        "for k, v in zl:\n",
        "    bag_of_words(k)\n",
        "    Final_Data.append((bag_of_words(k),v))\n",
        "\n",
        "import random\n",
        "random.shuffle(Final_Data)\n",
        "\n",
        "#splits the data around 70% of 500 *350 reviews* for both testing and training\n",
        "\n",
        "train_set, test_set = Final_Data[0:778], Final_Data[778:]\n",
        "\n",
        "#Now we will calculate accuracy, precision, recall, and f-measure using Naives Bayes classifier\n",
        "\n",
        "import nltk\n",
        "import collections\n",
        "from nltk.metrics.scores import (accuracy, precision, recall, f_measure)\n",
        "nb_classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "nb_classifier.show_most_informative_features(10)\n",
        "\n",
        "from nltk.classify.util import accuracy\n",
        "print(accuracy(nb_classifier, test_set))\n",
        "\n",
        "refsets = collections.defaultdict(set)\n",
        "testsets = collections.defaultdict(set)\n",
        "    \n",
        "for i, (Final_Data, label) in enumerate(test_set):\n",
        "    refsets[label].add(i)\n",
        "    observed = nb_classifier.classify(Final_Data)\n",
        "    testsets[observed].add(i)\n",
        "    \n",
        "print('acoso precision:', precision(refsets['acoso'], testsets['acoso']))\n",
        "print('acoso recall:', recall(refsets['acoso'], testsets['acoso']))\n",
        "print('acoso F-measure:', f_measure(refsets['acoso'], testsets['acoso']))\n",
        "print('noAcoso precision:', precision(refsets['noAcoso'], testsets['noAcoso']))\n",
        "print('noAcoso recall:', recall(refsets['noAcoso'], testsets['noAcoso']))\n",
        "print('noAcoso F-measure:', f_measure(refsets['noAcoso'], testsets['noAcoso']))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most Informative Features\n",
            "                  pedazo = True            acoso : noAcos =     16.9 : 1.0\n",
            "                    boca = True            acoso : noAcos =     11.1 : 1.0\n",
            "                 alguien = True           noAcos : acoso  =      8.1 : 1.0\n",
            "                      ur = True            acoso : noAcos =      7.2 : 1.0\n",
            "                   gente = True           noAcos : acoso  =      7.2 : 1.0\n",
            "                      .i = True           noAcos : acoso  =      7.2 : 1.0\n",
            "                    bajo = True            acoso : noAcos =      7.0 : 1.0\n",
            "                     amp = True           noAcos : acoso  =      6.7 : 1.0\n",
            "               retardado = True            acoso : noAcos =      6.6 : 1.0\n",
            "                 libtard = True            acoso : noAcos =      6.0 : 1.0\n",
            "0.7073170731707317\n",
            "acoso precision: 0.5899280575539568\n",
            "acoso recall: 0.7522935779816514\n",
            "acoso F-measure: 0.6612903225806451\n",
            "noAcoso precision: 0.8175675675675675\n",
            "noAcoso recall: 0.6797752808988764\n",
            "noAcoso F-measure: 0.7423312883435582\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXhwoYH_i5M-",
        "outputId": "375aba59-51e2-42a3-a423-ad505cbf76a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import collections\n",
        "from nltk import metrics\n",
        "from nltk.metrics.scores import (accuracy, precision, recall, f_measure)\n",
        "from nltk.classify import DecisionTreeClassifier\n",
        "from nltk.classify.util import accuracy\n",
        "dt_classifier = DecisionTreeClassifier.train(train_set, \n",
        "                                             binary=True, \n",
        "                                             entropy_cutoff=0.8, \n",
        "                                             depth_cutoff=5, \n",
        "                                             support_cutoff=30)\n",
        "from nltk.classify.util import accuracy\n",
        "print(accuracy(dt_classifier, test_set))\n",
        "\n",
        "refsets = collections.defaultdict(set)\n",
        "testsets = collections.defaultdict(set)\n",
        "    \n",
        "for i, (Final_Data, label) in enumerate(test_set):\n",
        "    refsets[label].add(i)\n",
        "    observed = dt_classifier.classify(Final_Data)\n",
        "    testsets[observed].add(i)\n",
        "    \n",
        "print('acoso precision:', precision(refsets['acoso'], testsets['acoso']))\n",
        "print('acoso recall:', recall(refsets['acoso'], testsets['acoso']))\n",
        "print('acoso F-measure:', f_measure(refsets['acoso'], testsets['acoso']))\n",
        "print('non-bullying precision:', precision(refsets['noAcoso'], testsets['noAcoso']))\n",
        "print('non-bullying recall:', recall(refsets['noAcoso'], testsets['noAcoso']))\n",
        "print('non-bullying F-measure:', f_measure(refsets['noAcoso'], testsets['noAcoso']))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7421602787456446\n",
            "acoso precision: 0.7611940298507462\n",
            "acoso recall: 0.46788990825688076\n",
            "acoso F-measure: 0.5795454545454545\n",
            "non-bullying precision: 0.7363636363636363\n",
            "non-bullying recall: 0.9101123595505618\n",
            "non-bullying F-measure: 0.8140703517587938\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqpfq9uVi7Bz",
        "outputId": "c2294f97-a68b-4382-e16e-add896daf3b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "#Create Logistic Regression model to compare\n",
        "from nltk.classify import MaxentClassifier\n",
        "import collections\n",
        "from nltk.metrics.scores import (accuracy, precision, recall, f_measure)\n",
        "\n",
        "logit_classifier = MaxentClassifier.train(train_set, algorithm='gis', trace=0, max_iter=10, min_lldelta=0.5)\n",
        " \n",
        "for i, (Final_Data, label) in enumerate(test_set):\n",
        "    refsets[label].add(i)\n",
        "    observed = logit_classifier.classify(Final_Data)\n",
        "    testsets[observed].add(i)\n",
        "  \n",
        "print('pos precision:', precision(refsets['acoso'], testsets['noAcoso']))\n",
        "print('pos recall:', recall(refsets['acoso'], testsets['noAcoso']))\n",
        "print('pos F-measure:', f_measure(refsets['acoso'], testsets['noAcoso']))\n",
        "print('neg precision:', precision(refsets['noAcoso'], testsets['noAcoso']))\n",
        "print('neg recall:', recall(refsets['noAcoso'], testsets['noAcoso']))\n",
        "print('neg F-measure:', f_measure(refsets['noAcoso'], testsets['noAcoso']))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos precision: 0.2938775510204082\n",
            "pos recall: 0.6605504587155964\n",
            "pos F-measure: 0.4067796610169492\n",
            "neg precision: 0.7061224489795919\n",
            "neg recall: 0.9719101123595506\n",
            "neg F-measure: 0.817966903073286\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-y357gli87i",
        "outputId": "372aac8f-0c1a-4436-9da8-a95f6f130bca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# SVM model\n",
        "\n",
        "from nltk.classify import SklearnClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "SVM_classifier = SklearnClassifier(SVC(), sparse=False).train(train_set)\n",
        "\n",
        "for i, (Final_Data, label) in enumerate(test_set):\n",
        "    refsets[label].add(i)\n",
        "    observed = SVM_classifier.classify(Final_Data)\n",
        "    testsets[observed].add(i)\n",
        "    \n",
        "print('pos precision:', precision(refsets['acoso'], testsets['acoso']))\n",
        "print('pos recall:', recall(refsets['acoso'], testsets['acoso']))\n",
        "print('pos F-measure:', f_measure(refsets['acoso'], testsets['acoso']))\n",
        "print('neg precision:', precision(refsets['noAcoso'], testsets['noAcoso']))\n",
        "print('neg recall:', recall(refsets['noAcoso'], testsets['noAcoso']))\n",
        "print('neg F-measure:', f_measure(refsets['noAcoso'], testsets['noAcoso']))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos precision: 0.6299212598425197\n",
            "pos recall: 0.7339449541284404\n",
            "pos F-measure: 0.6779661016949153\n",
            "neg precision: 0.7061224489795919\n",
            "neg recall: 0.9719101123595506\n",
            "neg F-measure: 0.817966903073286\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}